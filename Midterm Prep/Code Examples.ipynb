{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trails = pd.read_csv(\"/Users/owenk/OneDrive/Documents/GitHub/GSB_544_ML/Midterm Prep/national_park_trails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_trails(dataset, park_name, min_elev=None, max_elev=None, min_rating=None, trail_type=None):\n",
    "    # Filter by park name\n",
    "    trails = dataset[dataset['area_name'] == park_name]\n",
    "    \n",
    "    # Handle optional filters\n",
    "    if min_elev is not None:\n",
    "        trails = trails[trails['elevation_gain'] >= min_elev]\n",
    "    \n",
    "    if max_elev is not None:\n",
    "        trails = trails[trails['elevation_gain'] <= max_elev]\n",
    "    \n",
    "    if min_rating is not None:\n",
    "        trails = trails[trails['avg_rating'] >= min_rating]\n",
    "    \n",
    "    if trail_type is not None:\n",
    "        trails = trails[trails['route_type'] == trail_type]\n",
    "    \n",
    "    return trails[['name', 'length', 'elevation_gain', 'avg_rating', 'route_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>length</th>\n",
       "      <th>elevation_gain</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>route_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>Silversword Loop Via Halemau'u Trail</td>\n",
       "      <td>20116.750</td>\n",
       "      <td>1105.8144</td>\n",
       "      <td>4.5</td>\n",
       "      <td>loop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>Keonehe'ehe'e Trail</td>\n",
       "      <td>28324.384</td>\n",
       "      <td>1171.9560</td>\n",
       "      <td>5.0</td>\n",
       "      <td>out and back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>Kaupo Trail</td>\n",
       "      <td>19312.080</td>\n",
       "      <td>1670.9136</td>\n",
       "      <td>4.0</td>\n",
       "      <td>out and back</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name     length  ...  avg_rating    route_type\n",
       "3308  Silversword Loop Via Halemau'u Trail  20116.750  ...         4.5          loop\n",
       "3309                   Keonehe'ehe'e Trail  28324.384  ...         5.0  out and back\n",
       "3311                           Kaupo Trail  19312.080  ...         4.0  out and back\n",
       "\n",
       "[3 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_trails(trails, \"Haleakala National Park\", min_elev = 1000, min_rating = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_temps(month = None):\n",
    "  \n",
    "  my_url = \"__________\"\n",
    "  response = ___________(my_url)\n",
    "  soup = BeautifulSoup(_______, \"html.parser\")\n",
    "  \n",
    "    \n",
    "    \n",
    "  ## More of your code here\n",
    "  \n",
    "  \n",
    "  return ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_temps(month=None):\n",
    "    # Define the base URL\n",
    "    base_url = \"https://www.extremeweatherwatch.com/us-state-averages\"\n",
    "    \n",
    "    # Append month to the URL if provided\n",
    "    if month:\n",
    "        my_url = f\"{base_url}/{month.lower()}\"\n",
    "    else:\n",
    "        my_url = base_url\n",
    "    \n",
    "    # Get the page content\n",
    "    response = requests.get(my_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find the table containing the temperature data\n",
    "    table = soup.find(\"table\")\n",
    "    \n",
    "    # Extract the table headers (State, High, Low, Precipitation)\n",
    "    headers = [header.text for header in table.find_all(\"th\")]\n",
    "    \n",
    "    # Extract the rows of the table\n",
    "    rows = table.find_all(\"tr\")[1:]  # Skip header row\n",
    "    \n",
    "    # Parse each row to extract the data for each state\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = row.find_all(\"td\")\n",
    "        if cols:\n",
    "            data.append([col.text.strip() for col in cols])\n",
    "    \n",
    "    # Convert the data into a DataFrame\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m scrape_temps(month \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjanuary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m, in \u001b[0;36mscrape_temps\u001b[1;34m(month)\u001b[0m\n",
      "\u001b[0;32m     20\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Extract the table headers (State, High, Low, Precipitation)\u001b[39;00m\n",
      "\u001b[1;32m---> 23\u001b[0m headers \u001b[38;5;241m=\u001b[39m [header\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m header \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mth\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Extract the rows of the table\u001b[39;00m\n",
      "\u001b[0;32m     26\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip header row\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "scrape_temps(month = \"january\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# iterate over all rows in the faculty table\n",
    "for city in table.find_all(\"tr\")[2:]:\n",
    "\n",
    "    # Get all the cells () in the row.\n",
    "    cells = city.find_all(\"td\")\n",
    "\n",
    "    # The information we need is the text between tags.\n",
    "\n",
    "    city_tag = cells[0].find(\"strong\") or cells[0]\n",
    "    city = city_tag.get_text(strip=True)\n",
    "\n",
    "    state_tag = cells[1].find(\"a\") or cells[1]\n",
    "    state = state_tag.text\n",
    "\n",
    "    census_tag = cells[2].find(\"a\") or cells[2]\n",
    "    census_2020 = census_tag.get_text(strip=True)\n",
    "\n",
    "    area_tag = cells[5].find(\"a\") or cells[5]\n",
    "    area_2020 = area_tag.get_text(strip=True)\n",
    "\n",
    "    # Append this data.\n",
    "    rows.append({\n",
    "        \"city\": city,\n",
    "        \"state\": state,\n",
    "        \"census_2020\": census_2020,\n",
    "        \"area_2020\": area_2020\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
